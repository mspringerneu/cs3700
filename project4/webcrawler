#!/usr/bin/python

import sys
import socket
import time
import datetime
import select
import json
import math
import re
import os
from collections import deque
from urlparse import urlparse
from HTMLParser import HTMLParser


#########################
##  ILLEGAL LIBRARIES  ##
#########################

# urllib
# urllib2
# httplib
# requests
# pycurl
# cookielib

###########################
##  HIGH-LEVEL APPROACH  ##
###########################


    #### VARIABLES TO STORE ####


        ## PENDING: AN ARRAY OF URLS THE CRAWLER HAS YET TO EXPLORE
        ## DATABASES: AN ARRAY OF DATABASES MAINTAINED BY THE CRAWLER, SORTED BY LOCATION ON THE HASH CIRCLE


    #### CRAWLING ALGORITHM ####


        ## LOG IN
        ## 'GET' HOME PAGE HTML
        ## PARSE HOME PAGE HTML
        ## SCAN FOR <a href='fring.ccs.neu.edu/fakebook/xxxxxxxxx'></a> LINKS, APPEND THEM TO 'PENDING' ARRAY
        ## SEPARATE THE 9-DIGIT PROFILE ID FROM THE URL




#########################
##  HTTP STATUS CODES  ##
#########################


    ## 200 - All clear
    ## 301 - HTTP redirect: crawler should retry request using url returned in Location header
    ## 403 - Forbidden: crawler should abandon the URL that generated the error code
    ## 404 - Not Found: crawler should abandon the URL that generated the error code
    ## 500 - Internal Server Error: crawler should retry the request until successful


########################
##  GLOBAL VARIABLES  ##
########################

# username and password to log in to fakebook, to be given on program execution
username = ''
password = ''

# my login credentials that I felt like saving for god knows what reason
usernameMatt = '000057168'
passwordMatt = 'YG99AKBL'

# domain prefix for use in ensuring the webcrawler doesn't leave the page and for separating out the profile ids
domain = 'http://fring.ccs.neu.edu'
login = '/accounts/login/?next=/fakebook/'
home = '/fakebook/'

# variable for use in the profile id hashing function
hashCode = 256

# timeout variable
timeout = 0.3


########################
##  FAKEBOOK COOKIES  ##
########################

# Fakebook stores two (2) cookies:

# csrftoken

#########################################################
# Name:	csrftoken                                       #
# Content:	8f3d3ef726e08ca2473e3955b8cd1f89            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:03 PM     #
# Expires:	Sunday, November 5, 2017 at 12:52:03 PM     #
#########################################################

csrfToken = ''

# sessionid

#########################################################
# Name:	sessionid                                       #
# Content:	04fd7a109bb72170d319ef41bf3c39a3            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:17 PM     #
# Expires:	Sunday, November 20, 2016 at 12:52:17 PM    #
#########################################################

sessionIdToken = ''


# socket variables
socket.setdefaulttimeout = 0.50
os.environ['no_proxy'] = '127.0.0.1,localhost'
linkRegex = re.compile('<a\s*href=[\'|"](.*?)[\'"].*?>')
CR = "\r\n"
CRLF = "\r\n\r\n"


######################################################################
#   CLASS: WEBCRAWLER
#
#       ARGUMENTS
#
#           dbCount         - number of databases the webcrawler has
#
#       ADDITIONAL STORED VARIABLES
#
#           socket          - INET, STREAMing socket for handlingHTTP requests
#           parser          - HTML parser
#           frontier         - the deque of fakebook profile ids the crawler has yet to visit
#           databases       - the array of databases used by the web crawler, sorted by hash circle position
#           secretFlags     - array of secret flags scraped from fakebook pages
#
#       METHODS
#
#           crawl           - main processing loop for the webcrawler, returns array of secret flag strings
#           parsePageHTML   - GET and parse the HTML for a given URL
#           generateURL     - returns the fakebook profile page URL from a given fakebook profile id
#           hashId          - returns the hash of a given fakebook profile id
#           expandFrontier  - adds unvisited profile ids to the frontier list
#           markVisited     - mark a profileId as visited, storing it in the appropriate database
#

class WebCrawler:
    # class constructor
    def __init__(self, dbCount):
        self.dbCount = dbCount
        #self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.parser = CrawlerHTMLParser()
        self.frontier = deque()
        self.databases = []
        for i in range(dbCount):
            self.databases.append(Database(i, (i / float(dbCount))))
        self.secretFlags = []

    # returns true if the database contains the given profile id
    def crawl(self):
        # if there are still unvisited profile ids, continue crawling
        while len(self.frontier) > 0 and len(self.secretFlags) < 5:
            profileId = self.frontier[0]
            friendsList = []
            profileURL = currentUrl = domain + home + str(profileId) + "/"
            friendsURL = currentUrl = domain + home + str(profileId) + "/friends/1/"

            print profileURL
            profilePage = self.GET(currentUrl, True)
            parsedProfilePage = self.parseHTML(profilePage)

            # scan all of the user's 'friends' pages
            scanningFriends = True
            while scanningFriends:
                status = True
                print friendsURL
                friendsPage = self.GET(friendsURL, True)
                parsedFriendsPage = self.parseHTML(friendsPage)
                if len(parsedFriendsPage['nextURL']) > 0:
                    friendsURL = domain + parsedFriendsPage['nextURL']
                else:
                    scanningFriends = False
                for i in parsedFriendsPage['ids']:
                    friendsList.append(i)

            # add unvisited profile ids to the frontier
            self.expandFrontier(friendsList)
            # store the current profile id in the appropriate database
            self.markVisited(profileId)
            # pop the current profile id from the frontier
            self.frontier.popleft()

        # crawl has finished
        for i in self.secretFlags:
            print i
        sys.exit(0)

    # checks an array of profile ids, adding the unvisited ones to the frontier
    def expandFrontier(self, profileIds):
        for i in profileIds:
            self.handleProfileId(i)

    def markVisited(self, profileId):
        hashedID = self.hashId(profileId)
        print "Hashed ID for profile ID " + str(profileId) + ": " + str(hashedID)
        db = self.getDatabase(hashedID)
        print "Storing profile ID " + str(profileId) + " in database with ID " + str(db.id)
        db.markVisited(profileId)

    # evaluate a profile id.  If it has not been visited, add it to the frontier, otherwise do nothing
    def handleProfileId(self, profileId):
        hashedID = self.hashId(profileId)
        print "Hashed ID for " + str(profileId) + ": " + str(hashedID)
        db = self.getDatabase(hashedID)
        print "Checking database with ID " + str(db.id)
        if db.hasVisited(hashedID):
            print "Profile ID " + str(profileId) + " already visited.  Ignoring..."
        else:
            print "Profile ID " + str(profileId) + " has not been visited.  Expanding the frontier..."
            self.frontier.append(profileId)

    # handle various HTTP status codes
    def getHTTPStatusCode(self, header):
        status = re.search("HTTP/1.1 (.*)\r\n", header)
        if not status == None:
            status = re.split(' ', status.group(1))[0]
            print "HTTP status code: " + status
            return int(status)
        else:
            print "HTTP status code could not be parsed. Retrying HTTP request..."
            return -1

    # return the fakebook url for a given profile id
    def generateURL(self, profileId):
        global domain
        url = domain + str(profileId) + '/'
        return url

    # return the hashed value of a given profile id
    def hashId(self, profileId):
        global hashCode
        # produces a floating point number between 0 and 1
        return (profileId % hashCode) / float(hashCode)

    # return the id of the database a hashedId should use
    def getDatabase(self, hashedId):
        dbId = math.ceil(hashedId / 1/ float(self.dbCount)) % self.dbCount
        for i in self.databases:
            if i.id == dbId:
                return i

    # return the parsed HTML for a given URL
    def parseHTML(self, html):
        # parse page HTML, return in format {"ids": [array], "s_flag": string (optional)}
        parsed = self.parser.crawl(html)
        self.parser.reset()
        # if the page contained a secret_flag
        if len(parsed['s_flag']) > 0:
            self.secretFlags.append(parsed['s_flag'])
        return parsed

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def GET(self, pageUrl, cookies):
        global CRLF
        global CR
        global csrfToken
        global sessionIdToken

        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server
        # create an INET, STREAMing socket

        timer = time.time()

        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        """
        ***********************************************************************************
        * Note that the connect() operation is subject to the timeout setting,
        * and in general it is recommended to call settimeout() before calling connect()
        * or pass a timeout parameter to create_connection().
        * The system network stack may return a connection timeout error of its own
        * regardless of any Python socket timeout setting.
        ***********************************************************************************
        """
        s.settimeout(0.30)
        """
        **************************************************************************************
        * Avoid socket.error: [Errno 98] Address already in use exception
        * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
        * without waiting for its natural timeout to expire.
        **************************************************************************************
        """
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # s.setblocking(0)
        s.connect((HOST, PORT))

        if cookies:
            sendMsg = ('GET %s HTTP/1.1%s'
                       'Host: fring.ccs.neu.edu%s'
                       'Connection: Close%s'
                       'Cookie: csrftoken=%s; sessionid=%s%s' % (path, CR, CR, CR, csrfToken, sessionIdToken, CRLF))
        else:
            sendMsg = ('GET %s HTTP/1.1%s'
                       'Host: fring.ccs.neu.edu%s'
                       'Connection: Close%s' % (path, CR, CR, CRLF))

        print sendMsg

        corrupt = True
        data = ''
        while corrupt:
            s.send(sendMsg)
            data = (s.recv(1000000))
            print data
            statusCode = self.getHTTPStatusCode(data)
            if statusCode == 200:
                corrupt = False
            elif statusCode == 403:
                print "HTTP/1.1 403 Forbidden Error, webcrawler terminating..."
                sys.exit(1)
            elif statusCode == 404:
                print "HTTP/1.1 404 Page Not Found Error, webcrawler terminating..."
                sys.exit(1)
            else:
                continue
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting
        s.shutdown(1)
        s.close()
        return data
        # print 'Received', repr(data)

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def POST(self, msg, pageUrl, cookies):
        global CR
        global CRLF
        global csrfToken
        global sessionIdToken

        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server
        # create an INET, STREAMing socket
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        """
        ***********************************************************************************
        * Note that the connect() operation is subject to the timeout setting,
        * and in general it is recommended to call settimeout() before calling connect()
        * or pass a timeout parameter to create_connection().
        * The system network stack may return a connection timeout error of its own
        * regardless of any Python socket timeout setting.
        ***********************************************************************************
        """
        s.settimeout(0.30)
        """
        **************************************************************************************
        * Avoid socket.error: [Errno 98] Address already in use exception
        * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
        * without waiting for its natural timeout to expire.
        **************************************************************************************
        """
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # s.setblocking(0)
        s.connect((HOST, PORT))
        corrupt = True
        data = ''
        while corrupt:
            s.send(msg)
            data = (s.recv(1000000))
            print data
            statusCode = self.getHTTPStatusCode(data)
            if statusCode == 302:
                corrupt = False
            elif statusCode == 403:
                print "HTTP/1.1 403 Forbidden Error, webcrawler terminating..."
                sys.exit(1)
            elif statusCode == 404:
                print "HTTP/1.1 404 Page Not Found Error, webcrawler terminating..."
                sys.exit(1)
            else:
                continue
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting
        s.shutdown(1)
        s.close()
        # print 'Received', repr(data)
        return data

    def extractCSRF(self, data):
        global csrfToken
        token = re.search("csrftoken=(.*); ex", data)
        #token = re.split(';', token.group(0))[0]
        csrfToken = token.group(1)

    def extractSessionId(self, data):
        global sessionIdToken
        token = re.search("sessionid=(.*); ex", data)
        #token = re.split(';', token.group(0))[0]
        sessionIdToken = token.group(1)

    def login(self):
        global domain
        global login
        global home
        global username
        global password
        global csrfToken
        global sessionIdToken

        url = domain + login

        print "URL: " + url

        data = self.GET(url, False)
        self.extractCSRF(data)
        self.extractSessionId(data)

        print "CSRF Token: " + csrfToken
        print "Session ID Token: " + sessionIdToken

        loginPost = ('POST /accounts/login/?next=/fakebook/ HTTP/1.1%s'
                     'Host: fring.ccs.neu.edu%s'
                     'Connection: Close%s'
                     'Cookie: csrftoken=%s; sessionid=%s%s'
                     'Content-Type: application/x-www-form-urlencoded%s'
                     'Content-Length: 105%s'
                     'username=000057168&password=YG99AKBL&csrfmiddlewaretoken=%s&next=/fakebook/%s' % (
                     CR, CR, CR, csrfToken, sessionIdToken, CR, CR, CRLF, csrfToken, CRLF))

        print "POST message: " + loginPost

        data = self.POST(loginPost, url, False)
        self.extractSessionId(data)
        print "Authenticated Session ID Token: " + sessionIdToken

        data = self.GET(domain + home, True)
        return data

    def initializeCrawler(self):
        homepage = self.login()
        crawled_homepage = self.parser.crawl(homepage)
        if len(crawled_homepage['s_flag']) > 0:
            self.secretFlags.append(crawled_homepage['s_flag'])
        if crawled_homepage['ids']:
            for i in crawled_homepage['ids']:
                self.frontier.append(i)
        print self.frontier


######################################################################
#   CLASS: DATABASE
#
#       ARGUMENTS
#
#           id              - the id of this database
#           position        - the position of the database on the hash circle (0.0-1.0)
#
#       ADDITIONAL STORED VARIABLES
#
#           visited         - the array of fakebook profile ids the crawler has visited
#
#       METHODS
#
#           hasVisited      - returns true if visited[] contains the given fakebook profile id
#

class Database:
    # class constructor
    def __init__(self, id, pos):
        self.id = id
        self.position = pos
        self.visited = []

    # method that returns true if the database contains the given profile id
    def hasVisited(self, profileId):
        containsId = False
        for i in range(len(self.visited)):
            if self.visited[i] == profileId:
                containsId = True
                break
        return containsId

    # inserts a 'visited' profile id in the visited array, returns True if successfully inserted
    def markVisited(self, profileId):
        inserted = False
        for i in range(len(self.visited)):
            if self.visited[i] > profileId:
                self.visited.insert(i, profileId)
                inserted = True
                break
        return inserted


######################################################################
#   CLASS: CRAWLERHTMLPARSER FROM HTMLPARSER
#
#       ADDITIONAL STORED VARIABLES
#
#           ids             - the array of fakebook profile ids the crawler has visited
#           flag            - secret_flag string (if any)
#           scanIds         - boolean that is True whilst within a 'ul' tag, False otherwise
#           scanSFs         - boolean that is True whilst in a <h2 class='secret_flag> tag, False otherwise
#
#       METHODS (TO OVERRIDE)
#
#           handle_starttag - what parser should do when it reads a start tag   (ex: "<div id='one'>")
#           handle_endtag   - what parser should do when it reads a start tag   (ex: "</div>")
#           handle_data     - what parser should do when it reads some data     (ex: "<tag>data</tag>")
#

class CrawlerHTMLParser(HTMLParser):
    def __init__(self):
        self.reset()

    # Override
    def handle_starttag(self, tag, attrs):
        # if we encounter a 'ul' start tag, we are now scanning the list of profiles
        if tag == 'ul':
            for attr, value in attrs:
                if (attr, value) == ('id', 'pagelist'):
                    self.scanNextURL = True
            if not self.scanNextURL:
                self.scanIds = True
        # if we encounter an 'a' tag and we are within a 'ul' tag, append the profile id
        elif tag == 'a':
            if self.scanIds:
                for attr, value in attrs:
                    if attr == 'href':
                        id = re.split('/', value)[2]
                        print id
                        #assert len(id) == 9 or len(id) == 8
                        self.ids.append(int(id))
            elif self.scanNextURL:
                for attr, value in attrs:
                    if attr == 'href':
                        self.nextURL = value
        elif tag == 'h2':
            for attr, value in attrs:
                if (attr, value) == ('class', 'secret_flag'):
                    self.scanSFs = True
        elif tag == 'html':
            self.scanHeader = False
        # print "Encountered a start tag:", tag

    # Override
    def handle_endtag(self, tag):
        if tag == 'ul':
            if self.scanIds:
                self.scanIds = False
            elif self.scanNextURL:
                self.scanNextURL = False
        elif tag == 'h2':
            if self.scanSFs:
                self.scanSFs = False
        # print "Encountered an end tag :", tag

    # Override
    def handle_data(self, data):
        if self.scanHeader:
            self.header = data
        elif self.scanSFs:
            self.flag = data
        elif self.scanNextURL:
            if data == 'next':
                self.scanNextURL = False
                self.hasNext = True
        # print "Encountered some data  :", data

    # Override
    def reset(self):
        self.header = ''
        self.nextURL = ''
        self.ids = []
        self.flag = ''
        self.scanHeader = True
        self.scanNextURL = False
        self.hasNext = False
        self.scanIds = False
        self.scanSFs = False
        HTMLParser.reset(self)

    def hasSecretFlag(self):
        return len(self.flag) > 0

    def crawl(self, html):
        # process HTML
        self.feed(html)
        if not self.hasNext:
            self.nextURL = ''

        retVal = {
                    'header': self.header,
                    'ids': self.ids,
                    's_flag': self.flag,
                    'nextURL': self.nextURL
                  }
        print retVal
        return retVal


# socket logic - SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets

# 676323867
# 676323867

# main processing logic
def main():

    global username
    global password

    #initialize username and password from command line arguments
    print sys.argv
    if not len(sys.argv) == 3:
        print "fakebook webcrawler requires a username and a password, exiting..."
        sys.exit(1)
    else:
        username = sys.argv[1]
        password = sys.argv[2]

    # initialize crawler
    crawler = WebCrawler(10)
    crawler.initializeCrawler()
    crawler.crawl()
    """
    friendsURL = domain + home + str(crawler.frontier[0]) + "/friends/1/"
    page = crawler.GET(friendsURL, True)
    parsed = crawler.parser.crawl(page)
    print parsed
    crawler.getHTTPStatusCode(parsed['header'])
    crawler.getHTTPStatusCode(page)
    """


# run main()
main()

