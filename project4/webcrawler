#!/usr/bin/python

import sys
import socket
import time
import datetime
import select
import json
import math
import re
import os
import StringIO
import gzip
import zlib
from collections import deque
from urlparse import urlparse
from HTMLParser import HTMLParser


#########################
##  ILLEGAL LIBRARIES  ##
#########################

# urllib
# urllib2
# httplib
# requests
# pycurl
# cookielib

###########################
##  HIGH-LEVEL APPROACH  ##
###########################


    #### VARIABLES TO STORE ####


        ## PENDING: AN ARRAY OF URLS THE CRAWLER HAS YET TO EXPLORE
        ## DATABASES: AN ARRAY OF DATABASES MAINTAINED BY THE CRAWLER, SORTED BY LOCATION ON THE HASH CIRCLE


    #### CRAWLING ALGORITHM ####


        ## LOG IN
        ## 'GET' HOME PAGE HTML
        ## PARSE HOME PAGE HTML
        ## SCAN FOR <a href='fring.ccs.neu.edu/fakebook/xxxxxxxxx'></a> LINKS, APPEND THEM TO 'PENDING' ARRAY
        ## SEPARATE THE 9-DIGIT PROFILE ID FROM THE URL




#########################
##  HTTP STATUS CODES  ##
#########################


    ## 200 - All clear
    ## 301 - HTTP redirect: crawler should retry request using url returned in Location header
    ## 403 - Forbidden: crawler should abandon the URL that generated the error code
    ## 404 - Not Found: crawler should abandon the URL that generated the error code
    ## 500 - Internal Server Error: crawler should retry the request until successful


########################
##  GLOBAL VARIABLES  ##
########################

# username and password to log in to fakebook, to be given on program execution
username = ''
password = ''

# my login credentials that I felt like saving for god knows what reason
usernameMatt = '000057168'
passwordMatt = 'YG99AKBL'

# domain prefix for use in ensuring the webcrawler doesn't leave the page and for separating out the profile ids
domain = 'http://fring.ccs.neu.edu'
login = '/accounts/login/?next=/fakebook/'
home = '/fakebook/'

# variable for use in the profile id hashing function
hashCode = 256

# timeout variable
timeout = 0.3
timer = 0

########################
##  FAKEBOOK COOKIES  ##
########################

# Fakebook stores two (2) cookies:

# csrftoken

#########################################################
# Name:	csrftoken                                       #
# Content:	8f3d3ef726e08ca2473e3955b8cd1f89            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:03 PM     #
# Expires:	Sunday, November 5, 2017 at 12:52:03 PM     #
#########################################################

csrfToken = ''

# sessionid

#########################################################
# Name:	sessionid                                       #
# Content:	04fd7a109bb72170d319ef41bf3c39a3            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:17 PM     #
# Expires:	Sunday, November 20, 2016 at 12:52:17 PM    #
#########################################################

sessionIdToken = ''


# socket variables
socket.setdefaulttimeout = 0.50
os.environ['no_proxy'] = '127.0.0.1,localhost'
linkRegex = re.compile('<a\s*href=[\'|"](.*?)[\'"].*?>')
CR = "\r\n"
CRLF = "\r\n\r\n"


######################################################################
#   CLASS: WEBCRAWLER
#
#       ARGUMENTS
#
#           dbCount         - number of databases the webcrawler has
#
#       ADDITIONAL STORED VARIABLES
#
#           socket          - INET, STREAMing socket for handlingHTTP requests
#           parser          - HTML parser
#           frontier         - the deque of fakebook profile ids the crawler has yet to visit
#           databases       - the array of databases used by the web crawler, sorted by hash circle position
#           secretFlags     - array of secret flags scraped from fakebook pages
#
#       METHODS
#
#           crawl           - main processing loop for the webcrawler, returns array of secret flag strings
#           parsePageHTML   - GET and parse the HTML for a given URL
#           generateURL     - returns the fakebook profile page URL from a given fakebook profile id
#           hashId          - returns the hash of a given fakebook profile id
#           expandFrontier  - adds unvisited profile ids to the frontier list
#           markVisited     - mark a profileId as visited, storing it in the appropriate database
#

class WebCrawler:
    # class constructor
    def __init__(self, dbCount):
        self.dbCount = dbCount
        #self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.parser = CrawlerHTMLParser()
        self.frontier = deque()
        self.databases = []
        for i in range(dbCount):
            self.databases.append(Database(i, (i / float(dbCount))))
        self.secretFlags = []
        self.pagesParsed = 0
        self.profilesVisited = 0

    # returns true if the database contains the given profile id
    def crawl(self):
        global timer
        # if there are still unvisited profile ids, continue crawling
        while len(self.frontier) > 0 and len(self.secretFlags) < 5:
            profileId = self.frontier[0]
            friendsList = []
            profileURL = currentUrl = domain + home + str(profileId) + "/"
            friendsURL = currentUrl = domain + home + str(profileId) + "/friends/1/"

            #print profileURL
            profilePage = self.GET(currentUrl, True)
            self.scanForSecretFlag(profilePage['data'])
            parsedProfilePage = self.parseHTML(profilePage['data'])

            # scan all of the user's 'friends' pages
            scanningFriends = True
            while scanningFriends:
                status = True
                print friendsURL
                friendsPage = self.GET(friendsURL, True)
                self.scanForSecretFlag(friendsPage['data'])
                parsedFriendsPage = self.parseHTML(friendsPage['data'])
                if len(parsedFriendsPage['nextURL']) > 0:
                    friendsURL = domain + parsedFriendsPage['nextURL']
                else:
                    scanningFriends = False
                for i in parsedFriendsPage['ids']:
                    friendsList.append(i)

            # add unvisited profile ids to the frontier
            self.expandFrontier(friendsList)
            # store the current profile id in the appropriate database
            self.markVisited(profileId)
            # pop the current profile id from the frontier
            self.frontier.popleft()
            self.profilesVisited = self.profilesVisited + 1

        # crawl has finished
        """
        print "********************************"
        print "**                            **"
        print "**        SECRET FLAGS        **"
        print "**                            **"
        print "********************************"
        for i in self.secretFlags:
            print "Secret flag: " + i
        print "********************************"
        print "**                            **"
        print "**        SECRET FLAGS        **"
        print "**                            **"
        print "********************************"
        """
        print "Pages Parsed: " + str(self.pagesParsed)
        print "Profiles Visited: " + str(self.profilesVisited)
        executionTimer = time.time() - timer
        mins = math.floor(executionTimer / 60)
        secs = executionTimer % 60
        print "Execution time: " + str(mins) + "min, " + str(secs) + "secs"
        sys.exit(0)

    # checks an array of profile ids, adding the unvisited ones to the frontier
    def expandFrontier(self, profileIds):
        for i in profileIds:
            self.handleProfileId(i)

    def inFrontier(self, profileId):
        inFrontier = False
        for i in self.frontier:
            if i == profileId:
                inFrontier = True
        return inFrontier

    def markVisited(self, profileId):
        hashedID = self.hashId(profileId)
        #print "Hashed ID for profile ID " + str(profileId) + ": " + str(hashedID)
        dbId = self.getDatabase(hashedID)
        #print "Storing profile ID " + str(profileId) + " in database with ID " + str(dbId)
        inserted = self.databases[dbId].markVisited(profileId)
        assert inserted == True

    # evaluate a profile id.  If it has not been visited, add it to the frontier, otherwise do nothing
    def handleProfileId(self, profileId):
        hashedID = self.hashId(profileId)
        # print "Hashed ID for " + str(profileId) + ": " + str(hashedID)
        dbId = self.getDatabase(hashedID)
        # print "Checking database with ID " + str(db.id)
        if (not self.databases[dbId].hasVisited(profileId)) and (not self.inFrontier(profileId)):
            self.frontier.append(profileId)
        """
        if self.databases[dbId].hasVisited(profileId):
            # print "Profile ID " + str(profileId) + " already visited.  Ignoring..."
            # sys.exit(1)
        elif self.inFrontier(profileId):
            # print "Profile ID " + str(profileId) + " is already in the frontier.  Ignoring..."
        else:
            #print "Profile ID " + str(profileId) + " has not been visited.  Expanding the frontier..."
            self.frontier.append(profileId)
        """

    # handle various HTTP status codes
    def getHTTPStatusCode(self, header):
        status = re.search("HTTP/1.1 (.*)\r\n", header)
        if not status == None:
            status = re.split(' ', status.group(1))[0]
            #print "HTTP status code: " + status
            return int(status)
        else:
            #print "HTTP status code could not be parsed. Retrying HTTP request..."
            return -1

    def scanForSecretFlag(self, data):
        flag = re.search("FLAG: (.*)</h2>", data)
        if not flag == None:
            flag = flag.group(1)
            print flag
            sys.exit(5)
            return True
        else:
            return False

    # unzips gzipped data
    def unzip(self, response):
        global CRLF
        split = re.split(CRLF, response, 1)
        header = split[0]
        data = split[1]

        #print '********'
        #print "Header: " + header
        #print '********'
        #print "Data: " + data
        #print '********'

        encoding = re.search("Content-Encoding: (.*)\r\n", header)
        length = re.search("Content-Length: (.*)\r\n", header)
        if (not encoding == None) and (not length == None):
            encoding = re.split(' ', encoding.group(1))[0]
            length = int(length.group(1))

            if encoding == 'gzip':
                #print "Response received with 'gzip' encoding and length " + str(length) + ".  Decoding..."
                stream = StringIO.StringIO(data)
                gzipped = gzip.GzipFile(fileobj=stream)
                unzipped = gzipped.read()
                #print "Unzipped: " + unzipped

                return {"header": header, "data": unzipped}
            else:
                #print "Response received with unknown encoding.  Exiting..."
                sys.exit(2)
        else:
            #print "No encoding detected.  Proceeding..."
            return {"header": header, "data": data}

    # return the fakebook url for a given profile id
    def generateURL(self, profileId):
        global domain
        url = domain + str(profileId) + '/'
        return url

    # return the hashed value of a given profile id
    def hashId(self, profileId):
        global hashCode
        # produces a floating point number between 0 and 1
        return (profileId % hashCode) / float(hashCode)

    # return the id of the database a hashedId should use
    def getDatabase(self, hashedId):
        return int(math.ceil(hashedId / (1 / float(self.dbCount))) % self.dbCount)

    # return the parsed HTML for a given URL
    def parseHTML(self, html):
        # parse page HTML, return in format {"ids": [array], "s_flag": string (optional)}
        parsed = self.parser.crawl(html)
        self.pagesParsed = self.pagesParsed + 1
        self.parser.reset()
        # if the page contained a secret_flag
        if len(parsed['s_flag']) > 0:
            print "Located secret_flag: " + parsed['s_flag']
            sys.exit(5)
            self.secretFlags.append(parsed['s_flag'])
        return parsed

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def GET(self, pageUrl, cookies):
        global CRLF
        global CR
        global csrfToken
        global sessionIdToken

        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server
        # create an INET, STREAMing socket

        timer = time.time()



        if cookies:
            sendMsg = ('GET %s HTTP/1.1%s'
                       'Host: fring.ccs.neu.edu%s'
                       'Connection: Close%s'
                       'Accept-Encoding: gzip%s'
                       'Cookie: csrftoken=%s; sessionid=%s%s' % (path, CR, CR, CR, CR, csrfToken, sessionIdToken, CRLF))
        else:
            sendMsg = ('GET %s HTTP/1.1%s'
                       'Host: fring.ccs.neu.edu%s'
                       'Accept-Encoding: gzip%s'
                       'Connection: Close%s' % (path, CR, CR, CR, CRLF))

        #print sendMsg

        corrupt = True
        response = ''
        header = ''
        data = ''

        while corrupt:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            """
            ***********************************************************************************
            * Note that the connect() operation is subject to the timeout setting,
            * and in general it is recommended to call settimeout() before calling connect()
            * or pass a timeout parameter to create_connection().
            * The system network stack may return a connection timeout error of its own
            * regardless of any Python socket timeout setting.
            ***********************************************************************************
            """
            #s.settimeout(0.50)
            """
            **************************************************************************************
            * Avoid socket.error: [Errno 98] Address already in use exception
            * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
            * without waiting for its natural timeout to expire.
            **************************************************************************************
            """
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            # s.setblocking(0)
            s.connect((HOST, PORT))
            s.send(sendMsg)
            response = (s.recv(1000000))

            statusCode = self.getHTTPStatusCode(response)
            if statusCode == 200:
                data = self.unzip(response)
                #print data
                corrupt = False
                s.shutdown(1)
                s.close()
            elif statusCode == 403:
                print "HTTP/1.1 403 Forbidden Error, webcrawler terminating..."
                s.shutdown(1)
                s.close()
                sys.exit(1)
            elif statusCode == 404:
                print "HTTP/1.1 404 Page Not Found Error, webcrawler terminating..."
                s.shutdown(1)
                s.close()
                sys.exit(1)
            else:
                s.shutdown(1)
                s.close()
                continue
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting

        return data
        # print 'Received', repr(data)

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def POST(self, msg, pageUrl, cookies):
        global CR
        global CRLF
        global csrfToken
        global sessionIdToken

        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server

        corrupt = True
        response = ''
        header = ''
        data = ''

        while corrupt:
            # create an INET, STREAMing socket
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            """
            ***********************************************************************************
            * Note that the connect() operation is subject to the timeout setting,
            * and in general it is recommended to call settimeout() before calling connect()
            * or pass a timeout parameter to create_connection().
            * The system network stack may return a connection timeout error of its own
            * regardless of any Python socket timeout setting.
            ***********************************************************************************
            """
            s.settimeout(0.50)
            """
            **************************************************************************************
            * Avoid socket.error: [Errno 98] Address already in use exception
            * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
            * without waiting for its natural timeout to expire.
            **************************************************************************************
            """
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            # s.setblocking(0)
            s.connect((HOST, PORT))
            s.send(msg)
            response = (s.recv(1000000))

            #print header
            statusCode = self.getHTTPStatusCode(response)
            if statusCode == 302:
                data = self.unzip(response)

                #print data
                corrupt = False
                s.shutdown(1)
                s.close()
            elif statusCode == 403:
                print "HTTP/1.1 403 Forbidden Error, webcrawler terminating..."
                s.shutdown(1)
                s.close()
                sys.exit(1)
            elif statusCode == 404:
                print "HTTP/1.1 404 Page Not Found Error, webcrawler terminating..."
                s.shutdown(1)
                s.close()
                sys.exit(1)
            else:
                continue
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting

        # print 'Received', repr(data)
        return data

    def extractCSRF(self, data):
        global csrfToken
        #print "Extracting CSRF token from :" + data
        token = re.search("csrftoken=(.*); ex", data)
        #token = re.split(';', token.group(0))[0]
        csrfToken = token.group(1)

    def extractSessionId(self, data):
        global sessionIdToken
        #print "Extracting session ID token from :" + data
        token = re.search("sessionid=(.*); ex", data)
        #token = re.split(';', token.group(0))[0]
        sessionIdToken = token.group(1)

    def login(self):
        global domain
        global login
        global home
        global username
        global password
        global csrfToken
        global sessionIdToken

        url = domain + login

        #print "URL: " + url

        response = self.GET(url, False)
        self.extractCSRF(response['header'])
        self.extractSessionId(response['header'])

        print "CSRF Token: " + csrfToken
        print "Session ID Token: " + sessionIdToken

        loginPost = ('POST /accounts/login/?next=/fakebook/ HTTP/1.1%s'
                     'Host: fring.ccs.neu.edu%s'
                     'Connection: Close%s'
                     'Cookie: csrftoken=%s; sessionid=%s%s'
                     'Content-Type: application/x-www-form-urlencoded%s'
                     'Content-Length: 105%s'
                     'Accept-Encoding: gzip%s'
                     'username=000057168&password=YG99AKBL&csrfmiddlewaretoken=%s&next=/fakebook/%s' % (
                     CR, CR, CR, csrfToken, sessionIdToken, CR, CR, CR, CRLF, csrfToken, CRLF))

        #print "POST message: " + loginPost

        response = self.POST(loginPost, url, False)
        self.extractSessionId(response['header'])
        print "Authenticated Session ID Token: " + sessionIdToken

        response = self.GET(domain + home, True)
        return response

    def initializeCrawler(self):
        homepage = self.login()
        crawled_homepage = self.parser.crawl(homepage['data'])
        self.pagesParsed = self.pagesParsed + 1
        self.scanForSecretFlag(homepage['data'])
        """
        if len(crawled_homepage['s_flag']) > 0:
            self.secretFlags.append(crawled_homepage['s_flag'])
        """
        if crawled_homepage['ids']:
            for i in crawled_homepage['ids']:
                self.frontier.append(i)
        print "Initial frontier: " + str(self.frontier)


######################################################################
#   CLASS: DATABASE
#
#       ARGUMENTS
#
#           id              - the id of this database
#           position        - the position of the database on the hash circle (0.0-1.0)
#
#       ADDITIONAL STORED VARIABLES
#
#           visited         - the array of fakebook profile ids the crawler has visited
#
#       METHODS
#
#           hasVisited      - returns true if visited[] contains the given fakebook profile id
#

class Database:
    # class constructor
    def __init__(self, id, pos):
        self.id = id
        self.position = pos
        self.visited = []

    # method that returns true if the database contains the given profile id
    def hasVisited(self, profileId):
        containsId = False
        for i in self.visited:
            if i == profileId:
                containsId = True
                break
        return containsId

    # inserts a 'visited' profile id in the visited array, returns True if successfully inserted
    def markVisited(self, profileId):
        inserted = False
        #print "Stored profile IDs: " + str(self.visited)
        #print "Database " + str(self.id) + " storing profile ID " + str(profileId)
        for i in range(len(self.visited)):
            if int(self.visited[i]) > int(profileId):
                # print str(self.visited[i])
                self.visited.insert(i, profileId)
                inserted = True
                break
        if not inserted:
            self.visited.append(profileId)
            inserted = True
        #print "Stored profile IDs: " + str(self.visited)
        return inserted

######################################################################
#   CLASS: CRAWLERHTMLPARSER FROM HTMLPARSER
#
#       ADDITIONAL STORED VARIABLES
#
#           ids             - the array of fakebook profile ids the crawler has visited
#           flag            - secret_flag string (if any)
#           scanIds         - boolean that is True whilst within a 'ul' tag, False otherwise
#           scanSFs         - boolean that is True whilst in a <h2 class='secret_flag> tag, False otherwise
#
#       METHODS (TO OVERRIDE)
#
#           handle_starttag - what parser should do when it reads a start tag   (ex: "<div id='one'>")
#           handle_endtag   - what parser should do when it reads a start tag   (ex: "</div>")
#           handle_data     - what parser should do when it reads some data     (ex: "<tag>data</tag>")
#

class CrawlerHTMLParser(HTMLParser):
    def __init__(self):
        self.reset()

    # Override
    def handle_starttag(self, tag, attrs):
        # if we encounter a 'ul' start tag, we are now scanning the list of profiles
        if tag == 'ul':
            for attr, value in attrs:
                if (attr, value) == ('id', 'pagelist'):
                    self.scanNextURL = True
            if not self.scanNextURL:
                self.scanIds = True
        # if we encounter an 'a' tag and we are within a 'ul' tag, append the profile id
        elif tag == 'a':
            if self.scanIds:
                for attr, value in attrs:
                    if attr == 'href':
                        id = re.split('/', value)[2]
                        #print id
                        #assert len(id) == 9 or len(id) == 8
                        self.ids.append(int(id))
            elif self.scanNextURL:
                for attr, value in attrs:
                    if attr == 'href':
                        self.nextURL = value
        elif tag == 'h2':
            for attr, value in attrs:
                if attr == 'class' and value == 'secret_flag':
                    sys.exit(4)
                    #self.scanSFs = True
                    #self.flag = str(tag) + str(attrs)
        # print "Encountered a start tag:", tag

    # Override
    def handle_endtag(self, tag):
        if tag == 'ul':
            if self.scanIds:
                self.scanIds = False
            elif self.scanNextURL:
                self.scanNextURL = False
        """
        elif tag == 'h2':
            if self.scanSFs:
                self.flag = self.flag + str(tag)
                self.scanSFs = False
        """
        # print "Encountered an end tag :", tag

    # Override
    def handle_data(self, data):
        if "FLAG:" in data:
            print "Encountered secret_flag: " + data
            sys.exit(5)
            self.flag = data[6:]
        elif self.scanNextURL:
            if data == 'next':
                self.scanNextURL = False
                self.hasNext = True
        # print "Encountered some data  :", data

    # Override
    def reset(self):
        self.header = ''
        self.nextURL = ''
        self.ids = []
        self.flag = ''
        self.scanNextURL = False
        self.hasNext = False
        self.scanIds = False
        #self.scanSFs = False
        HTMLParser.reset(self)

    def crawl(self, html):
        # process HTML
        self.feed(html)
        if not self.hasNext:
            self.nextURL = ''

        retVal = {
                    'ids': self.ids,
                    's_flag': self.flag,
                    'nextURL': self.nextURL
                  }
        print retVal
        return retVal


# socket logic - SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets

# 676323867
# 676323867

# main processing logic
def main():

    global username
    global password
    global timer

    #initialize username and password from command line arguments
    print sys.argv
    if not len(sys.argv) == 3:
        print "fakebook webcrawler requires a username and a password, exiting..."
        sys.exit(-1)
    else:
        username = sys.argv[1]
        password = sys.argv[2]

    timer = time.time()

    # initialize crawler
    crawler = WebCrawler(10)
    crawler.initializeCrawler()
    crawler.crawl()
    """
    friendsURL = domain + home + str(crawler.frontier[0]) + "/friends/1/"
    page = crawler.GET(friendsURL, True)
    parsed = crawler.parser.crawl(page)
    print parsed
    crawler.getHTTPStatusCode(parsed['header'])
    crawler.getHTTPStatusCode(page)
    """


# run main()
main()

