#!/usr/bin/python

import sys
import socket
import time
import datetime
import select
import json
import urlparse
import math
from collections import deque
from HTMLParser import HTMLParser


###########################
##  HIGH-LEVEL APPROACH  ##
###########################


    #### VARIABLES TO STORE ####


        ## PENDING: AN ARRAY OF URLS THE CRAWLER HAS YET TO EXPLORE
        ## DATABASES: AN ARRAY OF DATABASES MAINTAINED BY THE CRAWLER, SORTED BY LOCATION ON THE HASH CIRCLE


    #### CRAWLING ALGORITHM ####


        ## LOG IN
        ## 'GET' HOME PAGE HTML
        ## PARSE HOME PAGE HTML
        ## SCAN FOR <a href='fring.ccs.neu.edu/fakebook/xxxxxxxxx'></a> LINKS, APPEND THEM TO 'PENDING' ARRAY
        ## SEPARATE THE 9-DIGIT PROFILE ID FROM THE URL




#########################
##  HTTP STATUS CODES  ##
#########################


    ## 200 - All clear
    ## 301 - HTTP redirect: crawler should retry request using url returned in Location header
    ## 403 - Forbidden: crawler should abandon the URL that generated the error code
    ## 404 - Not Found: crawler should abandon the URL that generated the error code
    ## 500 - Internal Server Error: crawler should retry the request until successful


########################
##  GLOBAL VARIABLES  ##
########################

# username and password to log in to fakebook, to be given on program execution
username = ''
password = ''

# my login credentials that I felt like saving for god knows what reason
usernameMatt = '000057168'
passwordMatt = 'YG99AKBL'

# domain prefix for use in ensuring the webcrawler doesn't leave the page and for separating out the profile ids
domain = 'http://fring.ccs.neu.edu/fakebook/'

# variable for use in the profile id hashing function
hashCode = 256


######################################################################
#   CLASS: WEBCRAWLER
#
#       ARGUMENTS
#
#           dbCount         - number of databases the webcrawler has
#
#       ADDITIONAL STORED VARIABLES
#
#           parser          - HTML parser
#           pending         - the deque of fakebook profile ids the crawler has yet to visit
#           databases       - the array of databases used by the web crawler, sorted by hash circle position
#           secretFlags     - array of secret flags scraped from fakebook pages
#
#       METHODS
#
#           crawl           - main processing loop for the webcrawler, returns array of secret flag strings
#           parsePageHTML   - GET and parse the HTML for a given URL
#           generateURL     - returns the fakebook profile page URL from a given fakebook profile id
#           hashId          - returns the hash of a given fakebook profile id
#           addPending      - adds unvisited profile ids to the pending list
#           markVisited     - mark a profileId as visited, storing it in the appropriate database
#

class WebCrawler:
    # class constructor
    def __init__(self, dbCount):
        self.dbCount = dbCount
        self.parser = CrawlerHTMLParser()
        self.pending = deque()
        self.databases = []
        for i in range(dbCount):
            self.databases.append(Database(i, (i / float(dbCount))))
        self.secretFlags = []

    # returns true if the database contains the given profile id
    def crawl(self):
        # if there are still unvisited profile ids, continue crawling
        while len(self.pending) > 0:

            # generate URL for id stored in pending[0]
            url = self.generateURL(self.pending[0])
            # pull page HTML for id stored in pending[0]
            parsedHTML = self.parseHTML(url)

    # return the fakebook url for a given profile id
    def generateURL(self, id):
        global domain
        url = domain + str(id) + '/'
        return url

    # return the hashed value of a given profile id
    def hashId(self, profileId):
        global hashCode
        return (profileId % hashCode) / float(hashCode)

    # return the id of the database a hashedId should use
    def getDatabase(self, hashedId):
        return math.ceil(hashedId / float(1/self.dbCount)) % self.dbCount

    #return the parsed HTML for a given URL
    def parseHTML(self, url):
        # GET page HTML
        html = ''
        # parse page HTML, return in format {"ids": [array], "s_flag": string (optional)}
        parsed = self.parser.crawl(html)
        # if the page contained a secret_flag
        if parsed['s_flag']:
            self.secretFlags.append(parsed['s_flag'])
        for i in parsed['ids']:
            hash = self.hashId(i)
            db = self.getDatabase(hash)
            if not self.databases[db].hasVisited(i):
                self.pending.append(i)


######################################################################
#   CLASS: DATABASE
#
#       ARGUMENTS
#
#           id              - the id of this database
#           position        - the position of the database on the hash circle (0.0-1.0)
#
#       ADDITIONAL STORED VARIABLES
#
#           visited         - the array of fakebook profile ids the crawler has visited
#
#       METHODS
#
#           hasVisited      - returns true if visited[] contains the given fakebook profile id
#

class Database:
    # class constructor
    def __init__(self, id, pos):
        self.id = id
        self.position = pos
        self.visited = []

    # method that returns true if the database contains the given profile id
    def hasVisited(self, profileId):
        containsId = False
        for i in range(len(self.visited)):
            if self.visited[i] == profileId:
                containsId = True
                break
        return containsId

    # inserts a 'visited' profile id in the visited array, returns True if successfully inserted
    def markVisited(self, profileId):
        inserted = False
        for i in range(len(self.visited)):
            if self.visited[i] > profileId:
                self.visited.insert(i, profileId)
                inserted = True
                break
        return inserted


######################################################################
#   CLASS: CRAWLERHTMLPARSER FROM HTMLPARSER
#
#       ADDITIONAL STORED VARIABLES
#
#           ids             - the array of fakebook profile ids the crawler has visited
#           flag            - secret_flag string (if any)
#           scanningIds     - boolean that is True whilst within a 'ul' tag, False otherwise
#
#       METHODS (TO OVERRIDE)
#
#           handle_starttag - what parser should do when it reads a start tag   (ex: "<div id='one'>")
#           handle_endtag   - what parser should do when it reads a start tag   (ex: "</div>")
#           handle_data     - what parser should do when it reads some data     (ex: "<tag>data</tag>")
#

class CrawlerHTMLParser(HTMLParser):
    def __init__(self):
        self.ids = []
        self.flag = ''
        self.scanningIds = False

    # Override
    def handle_starttag(self, tag, attrs):
        # if we encounter a 'ul' start tag, we are now scanning the list of profiles
        if tag == 'ul':
            self.scanningIds = True
        # if we encounter an 'a' tag and we are within a 'ul' tag, append the profile id
        if tag == 'a':
            if self.scanningIds:
                url = attrs.get('href')
                id = url.split('/')[1]
                assert len(id) == 9
                self.ids.append(int(id))
        print "Encountered a start tag:", tag

    # Override
    def handle_endtag(self, tag):
        if tag == 'ul':
            self.scanningIds = False
        print "Encountered an end tag :", tag

    # Override
    def handle_data(self, data):
        print "Encountered some data  :", data

    # Override
    def reset(self):
        self.ids = []
        self.flag = ''

    def hasSecretFlag(self):
        return len(self.flag) > 0

    def crawl(self, html):
        # process HTML
        self.feed(html)
        if self.hasSecretFlag():
            return {'ids': self.ids, 's_flag': self.flag}
        else:
            return {'ids': self.ids}


# main processing logic
def main():

    global username
    global password

    #initialize username and password from command line arguments
    if not len(sys.argv) == 2:
        print "fakebook webcrawler requires a username and a password, exiting..."
        sys.exit(1)
    else:
        username = sys.argv[0]
        password = sys.argv[1]



# run main()
main()

