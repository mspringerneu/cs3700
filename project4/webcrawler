#!/usr/bin/python

import sys
import socket
import time
import datetime
import select
import json
import math
import re
import os
from collections import deque
from urlparse import urlparse
from HTMLParser import HTMLParser


#########################
##  ILLEGAL LIBRARIES  ##
#########################

# urllib
# urllib2
# httplib
# requests
# pycurl
# cookielib

###########################
##  HIGH-LEVEL APPROACH  ##
###########################


    #### VARIABLES TO STORE ####


        ## PENDING: AN ARRAY OF URLS THE CRAWLER HAS YET TO EXPLORE
        ## DATABASES: AN ARRAY OF DATABASES MAINTAINED BY THE CRAWLER, SORTED BY LOCATION ON THE HASH CIRCLE


    #### CRAWLING ALGORITHM ####


        ## LOG IN
        ## 'GET' HOME PAGE HTML
        ## PARSE HOME PAGE HTML
        ## SCAN FOR <a href='fring.ccs.neu.edu/fakebook/xxxxxxxxx'></a> LINKS, APPEND THEM TO 'PENDING' ARRAY
        ## SEPARATE THE 9-DIGIT PROFILE ID FROM THE URL




#########################
##  HTTP STATUS CODES  ##
#########################


    ## 200 - All clear
    ## 301 - HTTP redirect: crawler should retry request using url returned in Location header
    ## 403 - Forbidden: crawler should abandon the URL that generated the error code
    ## 404 - Not Found: crawler should abandon the URL that generated the error code
    ## 500 - Internal Server Error: crawler should retry the request until successful


########################
##  GLOBAL VARIABLES  ##
########################

# username and password to log in to fakebook, to be given on program execution
username = ''
password = ''

# my login credentials that I felt like saving for god knows what reason
usernameMatt = '000057168'
passwordMatt = 'YG99AKBL'

# domain prefix for use in ensuring the webcrawler doesn't leave the page and for separating out the profile ids
domain = 'http://fring.ccs.neu.edu'
login = '/accounts/login/?next=/fakebook/'

# variable for use in the profile id hashing function
hashCode = 256


########################
##  FAKEBOOK COOKIES  ##
########################

# Fakebook stores two (2) cookies:

# csrftoken

#########################################################
# Name:	csrftoken                                       #
# Content:	8f3d3ef726e08ca2473e3955b8cd1f89            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:03 PM     #
# Expires:	Sunday, November 5, 2017 at 12:52:03 PM     #
#########################################################

csrfToken = ''

# sessionid

#########################################################
# Name:	sessionid                                       #
# Content:	04fd7a109bb72170d319ef41bf3c39a3            #
# Domain:	fring.ccs.neu.edu                           #
# Path:	/                                               #
# Send for:	Any kind of connection                      #
# Accessible to script:	Yes                             #
# Created:	Sunday, November 6, 2016 at 12:52:17 PM     #
# Expires:	Sunday, November 20, 2016 at 12:52:17 PM    #
#########################################################

sessionIdToken = ''


# socket variables
socket.setdefaulttimeout = 0.50
os.environ['no_proxy'] = '127.0.0.1,localhost'
linkRegex = re.compile('<a\s*href=[\'|"](.*?)[\'"].*?>')
CR = "\r\n"
CRLF = "\r\n\r\n"


######################################################################
#   CLASS: WEBCRAWLER
#
#       ARGUMENTS
#
#           dbCount         - number of databases the webcrawler has
#
#       ADDITIONAL STORED VARIABLES
#
#           socket          - INET, STREAMing socket for handlingHTTP requests
#           parser          - HTML parser
#           pending         - the deque of fakebook profile ids the crawler has yet to visit
#           databases       - the array of databases used by the web crawler, sorted by hash circle position
#           secretFlags     - array of secret flags scraped from fakebook pages
#
#       METHODS
#
#           crawl           - main processing loop for the webcrawler, returns array of secret flag strings
#           parsePageHTML   - GET and parse the HTML for a given URL
#           generateURL     - returns the fakebook profile page URL from a given fakebook profile id
#           hashId          - returns the hash of a given fakebook profile id
#           addPending      - adds unvisited profile ids to the pending list
#           markVisited     - mark a profileId as visited, storing it in the appropriate database
#

class WebCrawler:
    # class constructor
    def __init__(self, dbCount):
        self.dbCount = dbCount
        #self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.parser = CrawlerHTMLParser()
        self.pending = deque()
        self.databases = []
        for i in range(dbCount):
            self.databases.append(Database(i, (i / float(dbCount))))
        self.secretFlags = []

    # returns true if the database contains the given profile id
    def crawl(self):
        # if there are still unvisited profile ids, continue crawling
        while len(self.pending) > 0:

            # generate URL for id stored in pending[0]
            url = self.generateURL(self.pending[0])
            # pull page HTML for id stored in pending[0]
            parsedHTML = self.parseHTML(url)

    # return the fakebook url for a given profile id
    def generateURL(self, id):
        global domain
        url = domain + str(id) + '/'
        return url

    # return the hashed value of a given profile id
    def hashId(self, profileId):
        global hashCode
        return (profileId % hashCode) / float(hashCode)

    # return the id of the database a hashedId should use
    def getDatabase(self, hashedId):
        return math.ceil(hashedId / float(1/self.dbCount)) % self.dbCount

    #return the parsed HTML for a given URL
    def parseHTML(self, url):
        # GET page HTML
        html = ''
        # parse page HTML, return in format {"ids": [array], "s_flag": string (optional)}
        parsed = self.parser.crawl(html)
        # if the page contained a secret_flag
        if parsed['s_flag']:
            self.secretFlags.append(parsed['s_flag'])
        for i in parsed['ids']:
            hash = self.hashId(i)
            db = self.getDatabase(hash)
            if not self.databases[db].hasVisited(i):
                self.pending.append(i)

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def GET(self, pageUrl):
        global CRLF
        global CR
        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server
        # create an INET, STREAMing socket
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        """
        ***********************************************************************************
        * Note that the connect() operation is subject to the timeout setting,
        * and in general it is recommended to call settimeout() before calling connect()
        * or pass a timeout parameter to create_connection().
        * The system network stack may return a connection timeout error of its own
        * regardless of any Python socket timeout setting.
        ***********************************************************************************
        """
        s.settimeout(0.30)
        """
        **************************************************************************************
        * Avoid socket.error: [Errno 98] Address already in use exception
        * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
        * without waiting for its natural timeout to expire.
        **************************************************************************************
        """
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # s.setblocking(0)
        s.connect((HOST, PORT))
        s.send("GET " + path + " HTTP/1.0%s" % (CRLF))
        data = (s.recv(1000000))
        print data
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting
        s.shutdown(1)
        s.close()
        return data
        # print 'Received', repr(data)

    # logic for HTTP GET requests
    # SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets
    def POST(self, msg, pageUrl):
        global CR
        global CRLF

        url = urlparse(pageUrl)
        path = url.path
        if path == "":
            path = "/"
        HOST = url.netloc  # The remote host
        PORT = 80  # The same port as used by the server
        # create an INET, STREAMing socket
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        """
        ***********************************************************************************
        * Note that the connect() operation is subject to the timeout setting,
        * and in general it is recommended to call settimeout() before calling connect()
        * or pass a timeout parameter to create_connection().
        * The system network stack may return a connection timeout error of its own
        * regardless of any Python socket timeout setting.
        ***********************************************************************************
        """
        s.settimeout(0.30)
        """
        **************************************************************************************
        * Avoid socket.error: [Errno 98] Address already in use exception
        * The SO_REUSEADDR flag tells the kernel to reuse a local socket in TIME_WAIT state,
        * without waiting for its natural timeout to expire.
        **************************************************************************************
        """
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # s.setblocking(0)
        s.connect((HOST, PORT))
        s.send(msg)
        data = (s.recv(1000000))
        print data
        # s.send(post)
        # data2 = (s.recv(1000000))
        # print data2
        # https://docs.python.org/2/howto/sockets.html#disconnecting
        s.shutdown(1)
        s.close()
        # print 'Received', repr(data)

    def extractCSRF(self, data):
        global csrfToken
        token = re.search("csrftoken=(.*);", data)
        token = re.split(';', token.group(0))[0]
        csrfToken = token

    def extractSessionId(self, data):
        global sessionIdToken
        token = re.search("sessionid=(.*);", data)
        token = re.split(';', token.group(0))[0]
        sessionIdToken = token

    def login(self):
        global domain
        global login
        global username
        global password

        url = domain + login

        data = self.GET(url)
        self.extractCSRF(data)
        self.extractSessionId(data)

        print csrfToken
        print sessionIdToken

        loginPost = ('POST /accounts/login/?next=/fakebook/ HTTP/1.0%s'
                     'Host: fring.ccs.neu.edu%s'
                     'Connection: Keep-Alive%s'
                     'Cookie: %s; %s%s'
                     'Content-Type: application/x-www-form-urlencoded%s'
                     'Content-Length: 105%s'
                     'username=000057168&password=YG99AKBL&csrfmiddlewaretoken=%s&next=/fakebook/%s' % (
                     CR, CR, CR, csrfToken, sessionIdToken, CR, CR, CRLF, csrfToken, CRLF))

        data = self.POST(loginPost, url)





######################################################################
#   CLASS: DATABASE
#
#       ARGUMENTS
#
#           id              - the id of this database
#           position        - the position of the database on the hash circle (0.0-1.0)
#
#       ADDITIONAL STORED VARIABLES
#
#           visited         - the array of fakebook profile ids the crawler has visited
#
#       METHODS
#
#           hasVisited      - returns true if visited[] contains the given fakebook profile id
#

class Database:
    # class constructor
    def __init__(self, id, pos):
        self.id = id
        self.position = pos
        self.visited = []

    # method that returns true if the database contains the given profile id
    def hasVisited(self, profileId):
        containsId = False
        for i in range(len(self.visited)):
            if self.visited[i] == profileId:
                containsId = True
                break
        return containsId

    # inserts a 'visited' profile id in the visited array, returns True if successfully inserted
    def markVisited(self, profileId):
        inserted = False
        for i in range(len(self.visited)):
            if self.visited[i] > profileId:
                self.visited.insert(i, profileId)
                inserted = True
                break
        return inserted


######################################################################
#   CLASS: CRAWLERHTMLPARSER FROM HTMLPARSER
#
#       ADDITIONAL STORED VARIABLES
#
#           ids             - the array of fakebook profile ids the crawler has visited
#           flag            - secret_flag string (if any)
#           scanIds         - boolean that is True whilst within a 'ul' tag, False otherwise
#           scanSFs         - boolean that is True whilst in a <h2 class='secret_flag> tag, False otherwise
#
#       METHODS (TO OVERRIDE)
#
#           handle_starttag - what parser should do when it reads a start tag   (ex: "<div id='one'>")
#           handle_endtag   - what parser should do when it reads a start tag   (ex: "</div>")
#           handle_data     - what parser should do when it reads some data     (ex: "<tag>data</tag>")
#

class CrawlerHTMLParser(HTMLParser):
    def __init__(self):
        self.ids = []
        self.flag = ''
        self.scanIds = False
        self.scanSFs = False

    # Override
    def handle_starttag(self, tag, attrs):
        # if we encounter a 'ul' start tag, we are now scanning the list of profiles
        if tag == 'ul':
            self.scanIds = True
        # if we encounter an 'a' tag and we are within a 'ul' tag, append the profile id
        elif tag == 'a':
            if self.scanIds:
                url = attrs.get('href')
                id = url.split('/')[1]
                assert len(id) == 9
                self.ids.append(int(id))
        elif tag == 'h2':
            if attrs.get('class') == 'secret_flag':
                self.scanSFs = True
        print "Encountered a start tag:", tag

    # Override
    def handle_endtag(self, tag):
        if tag == 'ul':
            self.scanIds = False
        elif tag == 'h2':
            if self.scanSFs:
                self.scanSFs = False
        print "Encountered an end tag :", tag

    # Override
    def handle_data(self, data):
        if self.scanSFs:
            self.flag = data
        print "Encountered some data  :", data

    # Override
    def reset(self):
        self.ids = []
        self.flag = ''

    def hasSecretFlag(self):
        return len(self.flag) > 0

    def crawl(self, html):
        # process HTML
        self.feed(html)
        if self.hasSecretFlag():
            return {'ids': self.ids, 's_flag': self.flag}
        else:
            return {'ids': self.ids}


# socket logic - SOURCE: http://stackoverflow.com/questions/5755507/creating-a-raw-http-request-with-sockets



# main processing logic
def main():

    global username
    global password

    #initialize username and password from command line arguments
    print sys.argv
    if not len(sys.argv) == 3:
        print "fakebook webcrawler requires a username and a password, exiting..."
        sys.exit(1)
    else:
        username = sys.argv[1]
        password = sys.argv[2]

    # initialize crawler
    crawler = WebCrawler(10)
    crawler.login()




# run main()
main()

